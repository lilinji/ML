# 2.1. 自然语言处理（NLP, Natural Language Processing）

自然语言处理(NLP) 是人工智能(AI) 的一个分支，它能够使计算机理解、生成和处理人类语言，支持用户使用自然语言文本或语音来询问(interrogate) 数据，因此又被称为“语言输入(language in)”。 

## 基本概念

<mark style={{backgroundColor:"orange"}}><b>自然语言处理（NLP）是计算机程序理解口语和书写的人类语言的能力，也称为自然语言</b></mark><mark style={{backgroundColor:"orange"}}>。</mark>NLP 已经存在了 50 多年，起源于语言学领域。它在许多领域具有各种实际应用，包括医学研究、搜索引擎和商业智能等。

* NLP 是什么
  * <mark style={{backgroundColor:"orange"}}><b>目标：使计算机能够像人类一样理解和输出自然语言</b></mark>（计算机只懂 0 和 1 的计算，而人类的语言则千变万化），特别是自动处理大规模自然语言语料
  * 交叉学科：计算机科学、人工智能和计算语言学
  * 难点：语言本身的复杂性、语境强相关、抽象概念联想、软硬件技术发展限制等
* NLP 解决的五个基本问题
  * **分类**：assigning a label to a string
    * 将载有信息的一篇文本映射到预先给定的某一类别或某几类别主题；
    * 比如把「太好吃了」这句话标记为正面情感，或者把「C罗职业生涯第 8 次帽子戏法」这篇文章归类到「体育」；
  * **匹配**：matching two strings
    * 研究两段文本之间关系的问题，因此我们广义的将**研究两段文本间关系**的问题定义为“文本匹配”问题；
    * 比如判定「感冒了是否要吃药？」和「感冒了要吃什么药？」是不一样的问题；
  * **翻译**：transforming one string to another
  * **结构化预测**：mapping string to structure
  * **马氏决策过程**：deciding next state given previous state and actions

更多阅读：

[Ben Lutkevich：自然语言处理 NLP](../%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3%E5%90%88%E9%9B%86/Ben-Lutkevich-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-NLP.md "mention")

[数据挖掘、机器学习、自然语言处理这三者是什么关系？这几个怎么入门啊？](../%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3%E5%90%88%E9%9B%86/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E8%BF%99%E4%B8%89%E8%80%85%E6%98%AF%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB-%E8%BF%99%E5%87%A0%E4%B8%AA%E6%80%8E%E4%B9%88%E5%85%A5%E9%97%A8%E5%95%8A.md "mention")



## 自然语言处理的范式转换

如果我们把时间线往前拉得更长一些，然后在更长的时间窗口内观察技术变迁及其影响，可能会更容易看清其中的一些关键节点。在最近 10 年来 NLP 领域的技术发展过程中，可能有三次比较大的研究范式转换。

* <mark style={{backgroundColor:"orange"}}><b>范式转换 1.0：深度学习引入前（ - 2013）</b></mark>
  * 从2001年的嵌入式词向量表示到2013年的word2vec，研究已经逐步解决传统词袋表征方式的无序性，并进一步丰富了表征向量的表达能力：以前人们通常通过经验主义，即特征工程的方式，或统计信息的方式来解决NLP问题，文本的表征也只是简单的词袋表示（Bag-of-Words），这种方式使得文本丢失了序列信息以及背景依赖信息等。
* <mark style={{backgroundColor:"orange"}}><b>范式转换 2.0：从深度学习到两阶段预训练模型（2013 - 2020）</b></mark>
  * 回顾过去，2013 年是NLP领域的一个拐点，此后研究和开发呈指数增长。word embedding 的进步最终激发了神经网络在NLP中的广泛应用。需要解决的关键挑战是在神经网络中允许可变长度的序列输入，这最终导致了三种架构的出现，即：循环神经网络（RNN）（很快被长短期记忆（LSTM）网络替换），卷积神经网络（CNN）和递归神经网络。
  * 而自从 2018 年，基于 Transformer 的 Bert/GPT 模型诞生后，出现了明显的技术统一趋向。首先，NLP中不同的子领域，其特征抽取器都逐渐从 LSTM/CNN 统一到 Transformer 上。更准确地说，NLP各种任务其实收敛到了两个不同的预训练模型框架里：
    * 对于自然语言理解类任务，其技术体系统一到了以Bert为代表的“双向语言模型预训练+应用Fine-tuning”模式；
    * 而对于自然语言生成类任务，其技术体系则统一到了以GPT 2.0为代表的“自回归语言模型（即从左到右单向语言模型）+Zero /Few Shot Prompt”模式。
* <mark style={{backgroundColor:"orange"}}><b>范式转换 3.0：从预训练模型走向通用人工智能 AGI（2020 - ）</b></mark>
  * ChatGPT是触发这次范型转换的关键节点，但是在InstructGPT出现之前，其实LLM处于这次范式转换前的一个过渡期。

相关阅读：

[NLP 模型发展简要史：从 bags of words 到 Transformer 家族](../%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3%E5%90%88%E9%9B%86/NLP-%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E7%AE%80%E8%A6%81%E5%8F%B2-%E4%BB%8E-bags-of-words-%E5%88%B0-Transformer-%E5%AE%B6%E6%97%8F.md "mention")

[强烈推荐】通向AGI之路：大型语言模型（LLM）技术精要](../%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3%E5%90%88%E9%9B%86/%E5%BC%BA%E7%83%88%E6%8E%A8%E8%8D%90-%E9%80%9A%E5%90%91AGI%E4%B9%8B%E8%B7%AF-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-LLM-%E6%8A%80%E6%9C%AF%E7%B2%BE%E8%A6%81.md "mention")
